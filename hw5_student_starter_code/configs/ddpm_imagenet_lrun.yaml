# Resume from 50 epochs and train to 200 total
run_name: ddpm_imagenet128_inference_with_cfg_learned_varianc_offset
num_epochs: 100  # Total epochs (will train epochs 51-200)

# Point to your last checkpoint
ckpt: /ocean/projects/cis250233p/hxin1/Deep-Learning-Diffusion/hw5_student_starter_code/experiments/ddpm_imagenet128_long_run_with_cfg_learned_varianc_offset/checkpoints/checkpoint_epoch_99.pth
seed: 42
data_dir: /ocean/projects/cis250233p/cchen36/Deep-Learning-Diffusion/hw5_student_starter_code/data/imagenet100_128x128/train
image_size: 128
batch_size: 32        # per GPU
num_workers: 8
num_classes: 100
#num_epochs: 100
learning_rate: 2e-4
weight_decay: 1e-4

num_train_timesteps: 1000
num_inference_steps: 250
num_inference_samples: 5000
samples_per_class: 50
beta_start: 1e-4
beta_end: 1e-2
beta_schedule: linear

variance_type: learned
predictor_type: epsilon

unet_in_size: 128
unet_in_ch: 3
unet_ch: 128
unet_num_res_blocks: 2
unet_ch_mult: [1, 2, 2, 4]   # four stages for 128Ã—128
unet_attn: [2, 3]            # add attention on top two scales
unet_dropout: 0

use_ddim: False
use_cfg: True               # flip to True later if you add class emb.
