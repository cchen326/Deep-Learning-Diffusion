10/30/2025 22:31:08 - INFO - __main__ - ***** Training arguments *****
10/30/2025 22:31:08 - INFO - __main__ - Namespace(config='configs/ddim_cifar10.yaml', data_dir='./data/cifar10/train', image_size=128, batch_size=16, num_workers=8, num_classes=100, run_name='ddim_cifar10', output_dir='experiments', num_epochs=200, learning_rate=0.0002, weight_decay=0.0001, grad_clip=1.0, seed=42, mixed_precision='none', num_train_timesteps=1000, num_inference_steps=250, beta_start=0.0001, beta_end=0.01, beta_schedule='linear', variance_type='fixed_small', prediction_type='epsilon', num_inference_samples=5000, samples_per_class=50, clip_sample=True, clip_sample_range=1.0, unet_in_size=128, unet_in_ch=3, unet_ch=128, unet_ch_mult=[1, 2, 2, 4], unet_attn=[2, 3], unet_num_res_blocks=2, unet_dropout=0.0, latent_ddpm=False, use_cfg=False, cfg_guidance_scale=2.0, use_ddim=False, ckpt=None, predictor_type='epsilon', distributed=False, world_size=1, rank=0, local_rank=0, device='cuda', total_batch_size=16, max_train_steps=625000)
10/30/2025 22:31:08 - INFO - __main__ - ***** Running training *****
10/30/2025 22:31:08 - INFO - __main__ -   Num examples = 50000
10/30/2025 22:31:08 - INFO - __main__ -   Num Epochs = 200
10/30/2025 22:31:08 - INFO - __main__ -   Instantaneous batch size per device = 16
10/30/2025 22:31:08 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
10/30/2025 22:31:08 - INFO - __main__ -   Total optimization steps per epoch 3125
10/30/2025 22:31:08 - INFO - __main__ -   Total optimization steps = 625000
  0%|          | 0/625000 [00:00<?, ?it/s]10/30/2025 22:31:08 - INFO - __main__ - Epoch 1/200
  0%|          | 1/625000 [00:14<2602:55:52, 14.99s/it]10/30/2025 22:31:23 - INFO - __main__ - Epoch 1/200, Step 0/3125, Loss 1.0006221532821655 (1.0006221532821655)
  0%|          | 101/625000 [00:25<19:08:05,  9.07it/s]10/30/2025 22:31:34 - INFO - __main__ - Epoch 1/200, Step 100/3125, Loss 0.11070352792739868 (0.5332781409125517)
  0%|          | 201/625000 [00:37<19:07:58,  9.07it/s]10/30/2025 22:31:45 - INFO - __main__ - Epoch 1/200, Step 200/3125, Loss 0.012873309664428234 (0.28312829115997945)
  0%|          | 301/625000 [00:48<19:06:50,  9.08it/s]10/30/2025 22:31:56 - INFO - __main__ - Epoch 1/200, Step 300/3125, Loss 0.00917847827076912 (0.19361171504928176)
  0%|          | 401/625000 [00:59<19:07:10,  9.07it/s]10/30/2025 22:32:07 - INFO - __main__ - Epoch 1/200, Step 400/3125, Loss 0.01037838589400053 (0.14817329662320136)
  0%|          | 501/625000 [01:10<19:07:44,  9.07it/s]10/30/2025 22:32:19 - INFO - __main__ - Epoch 1/200, Step 500/3125, Loss 0.007660573348402977 (0.12083677619792328)
  0%|          | 601/625000 [01:21<19:07:14,  9.07it/s]10/30/2025 22:32:30 - INFO - __main__ - Epoch 1/200, Step 600/3125, Loss 0.007611697074025869 (0.10230600258905732)
  0%|          | 701/625000 [01:32<19:07:12,  9.07it/s]10/30/2025 22:32:41 - INFO - __main__ - Epoch 1/200, Step 700/3125, Loss 0.006669593509286642 (0.08907769740321347)
  0%|          | 801/625000 [01:43<19:05:31,  9.08it/s]10/30/2025 22:32:52 - INFO - __main__ - Epoch 1/200, Step 800/3125, Loss 0.0069231088273227215 (0.07911069738270526)
  0%|          | 901/625000 [01:54<19:05:11,  9.08it/s]10/30/2025 22:33:03 - INFO - __main__ - Epoch 1/200, Step 900/3125, Loss 0.008915751241147518 (0.07125208048939655)
  0%|          | 1001/625000 [02:05<19:05:13,  9.08it/s]10/30/2025 22:33:14 - INFO - __main__ - Epoch 1/200, Step 1000/3125, Loss 0.008245350793004036 (0.06496019844667285)
  0%|          | 1101/625000 [02:16<19:04:47,  9.08it/s]10/30/2025 22:33:25 - INFO - __main__ - Epoch 1/200, Step 1100/3125, Loss 0.006170204840600491 (0.05980150779534967)
  0%|          | 1201/625000 [02:27<19:05:00,  9.08it/s]10/30/2025 22:33:36 - INFO - __main__ - Epoch 1/200, Step 1200/3125, Loss 0.00779867684468627 (0.05551295131708586)
  0%|          | 1301/625000 [02:38<19:04:46,  9.08it/s]10/30/2025 22:33:47 - INFO - __main__ - Epoch 1/200, Step 1300/3125, Loss 0.005391828715801239 (0.05186324595558893)
  0%|          | 1401/625000 [02:49<19:04:32,  9.08it/s]10/30/2025 22:33:58 - INFO - __main__ - Epoch 1/200, Step 1400/3125, Loss 0.0059013827703893185 (0.04872757432763621)
  0%|          | 1501/625000 [03:00<19:03:52,  9.08it/s]10/30/2025 22:34:09 - INFO - __main__ - Epoch 1/200, Step 1500/3125, Loss 0.003879288211464882 (0.046043281649257355)
  0%|          | 1601/625000 [03:11<19:04:11,  9.08it/s]10/30/2025 22:34:20 - INFO - __main__ - Epoch 1/200, Step 1600/3125, Loss 0.004704643972218037 (0.04361347192715153)
  0%|          | 1701/625000 [03:22<19:04:05,  9.08it/s]10/30/2025 22:34:31 - INFO - __main__ - Epoch 1/200, Step 1700/3125, Loss 0.012398680672049522 (0.04148612920959195)
  0%|          | 1801/625000 [03:33<19:03:45,  9.08it/s]10/30/2025 22:34:42 - INFO - __main__ - Epoch 1/200, Step 1800/3125, Loss 0.005028886254876852 (0.03960390146077093)
  0%|          | 1901/625000 [03:44<19:03:41,  9.08it/s]10/30/2025 22:34:53 - INFO - __main__ - Epoch 1/200, Step 1900/3125, Loss 0.007777947001159191 (0.037912844992078636)
  0%|          | 2001/625000 [03:55<19:03:18,  9.08it/s]10/30/2025 22:35:04 - INFO - __main__ - Epoch 1/200, Step 2000/3125, Loss 0.0040727402083575726 (0.036366871054303355)
  0%|          | 2101/625000 [04:06<19:03:21,  9.08it/s]10/30/2025 22:35:15 - INFO - __main__ - Epoch 1/200, Step 2100/3125, Loss 0.005544142797589302 (0.03499777548324081)
  0%|          | 2201/625000 [04:17<19:03:15,  9.08it/s]10/30/2025 22:35:26 - INFO - __main__ - Epoch 1/200, Step 2200/3125, Loss 0.004644087515771389 (0.03375579409040206)
  0%|          | 2301/625000 [04:28<19:03:15,  9.08it/s]10/30/2025 22:35:37 - INFO - __main__ - Epoch 1/200, Step 2300/3125, Loss 0.006961542181670666 (0.03263251473422914)
  0%|          | 2313/625000 [04:29<19:03:04,  9.08it/s]
